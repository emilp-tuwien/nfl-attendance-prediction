{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "383c5e18",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c7338c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import kagglehub\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9331a15c",
   "metadata": {},
   "source": [
    "# MERGING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "548eb25e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.11), please consider upgrading to the latest version (0.3.12).\n"
     ]
    }
   ],
   "source": [
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"sujaykapadnis/nfl-stadium-attendance-dataset\")\n",
    "\n",
    "# Load the dataframe from the file\n",
    "def load_data(name):\n",
    "    return pd.read_csv(f\"{path}/{name}\")\n",
    "\n",
    "attendance_df = load_data(\"attendance.csv\")\n",
    "standings_df = load_data(\"standings.csv\")\n",
    "games_df = load_data(\"games.csv\")\n",
    "\n",
    "attendance_weekly_df = attendance_df[['team', 'team_name', 'year', 'week', 'weekly_attendance']]\n",
    "\n",
    "attendance_df = attendance_df.drop(columns=['weekly_attendance'])\n",
    "\n",
    "### ATTENDANCE\n",
    "# year was shitted by one year, so that last years standings will have influence on attendance\n",
    "attendance_df['year'] = attendance_df['year'] + 1\n",
    "\n",
    "attendance_df = attendance_df.merge(attendance_weekly_df, on=['team', 'team_name', 'year', 'week'], how='left')\n",
    "\n",
    "\n",
    "### STANDINGS\n",
    "# similar like above for standings\n",
    "standings_df['year'] = standings_df['year'] + 1\n",
    "\n",
    "attendance_standings_df = pd.merge(attendance_df, standings_df, on=['team', 'team_name', 'year'])\n",
    "\n",
    "attendance_standings_df['team_name'] = attendance_standings_df['team'] + ' ' + attendance_standings_df['team_name']\n",
    "\n",
    "### GAMES\n",
    "games_df = games_df[games_df['week'].str.isnumeric()]\n",
    "games_df['week'] = games_df['week'].astype(int)\n",
    "\n",
    "### MERGE\n",
    "\n",
    "df1 = attendance_standings_df.merge(games_df, left_on=['year', 'week', 'team_name'], right_on=['year', 'week', 'home_team'], how='inner')\n",
    "df2 = attendance_standings_df.merge(games_df, left_on=['year', 'week', 'team_name'], right_on=['year', 'week', 'away_team'], how='inner')\n",
    "\n",
    "df = df1.merge(df2, on=['year', 'week', 'home_team', 'away_team'], how='inner', suffixes=('_home', '_away'))\n",
    "\n",
    "df = df.drop(columns=['home_team', 'away_team', 'weekly_attendance_away', 'team_away'])\n",
    "df = df.rename(columns={'team_home': 'city'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dd8f215f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"merged_unprocessed.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b54cd3",
   "metadata": {},
   "source": [
    "# PREPROCESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "394e7ad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/db/djqfj81d1vs_17ntzp2n867m0000gn/T/ipykernel_79476/315123936.py:21: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df = df.replace({ 'Playoffs': 1, 'No Playoffs': 0, 'Won Superbowl': 1, 'No Superbowl': 0 })\n",
      "/var/folders/db/djqfj81d1vs_17ntzp2n867m0000gn/T/ipykernel_79476/315123936.py:102: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  season_stats_df[(season_stats_df['team_name'] == 'Arizona Cardinals')][season_stats_df['year'] == 2019]\n"
     ]
    }
   ],
   "source": [
    "duplicated_columns = ['winner', 'tie', 'day', 'date', 'time', 'pts_win', 'pts_loss', 'yds_win', 'turnovers_win', 'yds_loss', 'turnovers_loss', 'home_team_name', 'home_team_city', 'away_team_name', 'away_team_city', 'loss']\n",
    "duplicated_columns_away = list(map(lambda x: x + '_away', duplicated_columns))\n",
    "duplicated_columns_home = list(map(lambda x: x + '_home', duplicated_columns))\n",
    "\n",
    "duplicated_columns_mapping = dict(zip(duplicated_columns_home, duplicated_columns))\n",
    "\n",
    "df = df.drop(columns=duplicated_columns_away)\n",
    "df = df.rename(columns=duplicated_columns_mapping)\n",
    "\n",
    "columns_to_drop = ['total_home', 'away_home', 'date', 'time', 'city']\n",
    "\n",
    "columns_to_rename = {\n",
    "    'home_home': 'home_attendance_last_year_home',\n",
    "    'away_home': 'away_attendance_last_year_home',\n",
    "    'home_away': 'home_attendance_last_year_away',\n",
    "    'away_away': 'away_attendance_last_year_away',\n",
    "}\n",
    "\n",
    "df = df.drop(columns=columns_to_drop)\n",
    "df = df.rename(columns=columns_to_rename)\n",
    "df = df.replace({ 'Playoffs': 1, 'No Playoffs': 0, 'Won Superbowl': 1, 'No Superbowl': 0 })\n",
    "\n",
    "df[['playoffs_away', 'sb_winner_away', 'playoffs_home', 'sb_winner_home']] = df[['playoffs_away', 'sb_winner_away', 'playoffs_home', 'sb_winner_home']].astype(int)\n",
    "\n",
    "\n",
    "season_stats_df = pd.DataFrame(columns=['year', 'week', 'team_name', 'points', 'yards', 'turnovers', 'win', 'loss', 'tie'], dtype=int)\n",
    "season_stats_df['team_name'] = season_stats_df['team_name'].astype(str)\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    winning_team = row['winner']    \n",
    "    losing_team = row['team_name_away'] if winning_team == row['team_name_home'] else row['team_name_home']\n",
    "\n",
    "    for team in [winning_team, losing_team]:\n",
    "        season_stats_df.loc[len(season_stats_df)] = {\n",
    "            'year': row['year'],\n",
    "            'week': row['week'],\n",
    "            'team_name': team,\n",
    "            'points': row['pts_win'] if team == winning_team else row['pts_loss'],\n",
    "            'yards': row['yds_win'] if team == winning_team else row['yds_loss'],\n",
    "            'turnovers': row['turnovers_win'] if team == winning_team else row['turnovers_loss'],\n",
    "            'win': 1 if team == winning_team and pd.isna(row['tie']) else 0,\n",
    "            'loss': 1 if team == losing_team and pd.isna(row['tie']) else 0,\n",
    "            'tie': 1 if not pd.isna(row['tie'])  else 0\n",
    "        }\n",
    "\n",
    "min_year = season_stats_df['year'].min()\n",
    "max_year = season_stats_df['year'].max()\n",
    "teams = season_stats_df['team_name'].unique()\n",
    "\n",
    "# Fill in missing weeks with 0s\n",
    "for team in teams:\n",
    "    for year in range(min_year, max_year + 1):\n",
    "        for week in range(1, 18):\n",
    "            if len(season_stats_df[(season_stats_df['team_name'] == team) & (season_stats_df['year'] == year) & (season_stats_df['week'] == week)]) == 0:\n",
    "                season_stats_df.loc[len(season_stats_df)] = {\n",
    "                    'year': year,\n",
    "                    'week': week,\n",
    "                    'team_name': team,\n",
    "                    'points': 0,\n",
    "                    'yards': 0,\n",
    "                    'turnovers': 0,\n",
    "                    'win': 0,\n",
    "                    'loss': 0,\n",
    "                    'tie': 0\n",
    "                }\n",
    "\n",
    "\n",
    "# Group by years, and calculate running totals\n",
    "season_stats_df = season_stats_df.sort_values(by=['team_name', 'year', 'week'])\n",
    "season_stats_df['points'] = season_stats_df['points'].astype(int)\n",
    "season_stats_df['yards'] = season_stats_df['yards'].astype(int)\n",
    "season_stats_df['turnovers'] = season_stats_df['turnovers'].astype(int)\n",
    "season_stats_df['win'] = season_stats_df['win'].astype(int)\n",
    "season_stats_df['loss'] = season_stats_df['loss'].astype(int)\n",
    "season_stats_df['tie'] = season_stats_df['tie'].astype(int)\n",
    "\n",
    "season_stats_df['points'] = season_stats_df.groupby(['team_name', 'year'])['points'].cumsum()\n",
    "season_stats_df['yards'] = season_stats_df.groupby(['team_name', 'year'])['yards'].cumsum()\n",
    "season_stats_df['turnovers'] = season_stats_df.groupby(['team_name', 'year'])['turnovers'].cumsum()\n",
    "season_stats_df['win'] = season_stats_df.groupby(['team_name', 'year'])['win'].cumsum()\n",
    "season_stats_df['loss'] = season_stats_df.groupby(['team_name', 'year'])['loss'].cumsum()\n",
    "season_stats_df['tie'] = season_stats_df.groupby(['team_name', 'year'])['tie'].cumsum()\n",
    "\n",
    "\n",
    "season_stats_df['week'] = season_stats_df['week'] + 1\n",
    "# season_stats_df = season_stats_df[season_stats_df['week'] <= 17]\n",
    "\n",
    "for index, row in season_stats_df[season_stats_df['week'] == 18].iterrows():\n",
    "    season_stats_df.loc[index] = {\n",
    "        'year': row['year'],\n",
    "        'week': 1,\n",
    "        'team_name': row['team_name'],\n",
    "        'points': 0,\n",
    "        'yards': 0,\n",
    "        'turnovers': 0,\n",
    "        'win': 0,\n",
    "        'loss': 0,\n",
    "        'tie': 0\n",
    "    }\n",
    "\n",
    "season_stats_df = season_stats_df.sort_values(by=['team_name', 'year', 'week'])\n",
    "season_stats_df[(season_stats_df['team_name'] == 'Arizona Cardinals')][season_stats_df['year'] == 2019]\n",
    "games_columns_to_delete = ['winner', 'tie', 'day', 'pts_win', 'pts_loss', 'yds_win', 'turnovers_win', 'yds_loss', 'turnovers_loss', 'home_team_name', 'home_team_city', 'away_team_name', 'away_team_city']\n",
    "df = df.drop(columns=games_columns_to_delete)\n",
    "\n",
    "season_stats_df = season_stats_df.rename(columns={ 'team_name': 'team_name_home' })\n",
    "df = df.merge(season_stats_df, on=['team_name_home', 'year', 'week'], how='inner', suffixes=['', '_home'])\n",
    "\n",
    "season_stats_df = season_stats_df.rename(columns={ 'team_name_home': 'team_name_away' })\n",
    "df = df.merge(season_stats_df, on=['team_name_away', 'year', 'week'], how='inner', suffixes=['', '_away'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7169ecc",
   "metadata": {},
   "source": [
    "### Dataset Description\n",
    "\n",
    "The table below describes the columns resulting from the merging and preprocessing of NFL game data. These features will be used to train a Random Forest regressor to predict home game attendance.\n",
    "\n",
    "| Column Name                    | Description                                                                 |\n",
    "|-------------------------------|-----------------------------------------------------------------------------|\n",
    "| `team_name_home`              | Name of the home team                                                       |\n",
    "| `team_name_away`              | Name of the away team                                                       |\n",
    "| `year`                        | Year the game was played                                                    |\n",
    "| `week`                        | Week of the season (1–17 regular season)                                    |\n",
    "| `weekly_attendance_home`      | **Target variable** – Number of attendees at the home game                  |\n",
    "| `points_for_home`             | Total points scored by the home team throughout the season                  |\n",
    "| `points_against_home`         | Total points allowed by the home team throughout the season                 |\n",
    "| `points_differential_home`    | Point difference for the home team across the season                        |\n",
    "| `simple_rating_home`          | Home team's overall performance rating, factoring in strength of schedule  |\n",
    "| `margin_of_victory_home`      | Average margin of victory for the home team                                 |\n",
    "| `wins_home`                   | Total wins by the home team                                                 |\n",
    "| `offensive_ranking_home`      | Offensive performance rank of the home team                                 |\n",
    "| `defensive_ranking_home`      | Defensive performance rank of the home team                                 |\n",
    "| `points_for_away`             | Total points scored by the away team throughout the season                  |\n",
    "| `points_against_away`         | Total points allowed by the away team throughout the season                 |\n",
    "| `points_differential_away`    | Point difference for the away team across the season                        |\n",
    "| `simple_rating_away`          | Away team's overall performance rating                                      |\n",
    "| `wins_away`                   | Total wins by the away team                                                 |\n",
    "| `offensive_ranking_away`      | Offensive performance rank of the away team                                 |\n",
    "| `defensive_ranking_away`      | Defensive performance rank of the away team                                 |\n",
    "\n",
    "**Note**: Points and performance stats are aggregated across the season for both home and away teams. These variables are useful in understanding the potential drivers of attendance trends.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1421ceb",
   "metadata": {},
   "source": [
    "## Creating Attendance IDs for Splitting the Data into Train, Validation and Test for DBRepo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "65a162e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "\n",
    "df['attendance_id'] = df.index\n",
    "\n",
    "\n",
    "cols = ['attendance_id'] + [col for col in df.columns if col != 'attendance_id']\n",
    "df = df[cols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "86228a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('merged_preprocessed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6408880c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dbrepo.RestClient import RestClient\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv() \n",
    "password = os.getenv(\"DBREPO_PASSWORD\")\n",
    "\n",
    "client = RestClient(\n",
    "    endpoint=\"https://test.dbrepo.tuwien.ac.at\",\n",
    "    username=\"emilp-tuwien\",\n",
    "    password=password\n",
    ")\n",
    "training = client.get_identifier_data(identifier_id=\"56171866-21d8-4b89-a44c-a3044bf2d43d\")\n",
    "validation = client.get_identifier_data(identifier_id=\"e336829d-fefb-4a09-95f9-0196ee3fc194\")\n",
    "test =  client.get_identifier_data(identifier_id=\"a8b213d1-c66b-417d-8fc0-aa2bb9393ad2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "98618092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3365 entries, 0 to 3364\n",
      "Data columns (total 45 columns):\n",
      " #   Column                          Non-Null Count  Dtype \n",
      "---  ------                          --------------  ----- \n",
      " 0   attendance_id                   3365 non-null   object\n",
      " 1   team_name_home                  3365 non-null   object\n",
      " 2   year                            3365 non-null   object\n",
      " 3   home_attendance_last_year_home  3365 non-null   object\n",
      " 4   week                            3365 non-null   object\n",
      " 5   weekly_attendance_home          3365 non-null   object\n",
      " 6   wins_home                       3365 non-null   object\n",
      " 7   loss                            3365 non-null   object\n",
      " 8   points_for_home                 3365 non-null   object\n",
      " 9   points_against_home             3365 non-null   object\n",
      " 10  points_differential_home        3365 non-null   object\n",
      " 11  margin_of_victory_home          3365 non-null   object\n",
      " 12  strength_of_schedule_home       3365 non-null   object\n",
      " 13  simple_rating_home              3365 non-null   object\n",
      " 14  offensive_ranking_home          3365 non-null   object\n",
      " 15  defensive_ranking_home          3365 non-null   object\n",
      " 16  playoffs_home                   3365 non-null   object\n",
      " 17  sb_winner_home                  3365 non-null   object\n",
      " 18  team_name_away                  3365 non-null   object\n",
      " 19  total_away                      3365 non-null   object\n",
      " 20  home_attendance_last_year_away  3365 non-null   object\n",
      " 21  away_attendance_last_year_away  3365 non-null   object\n",
      " 22  wins_away                       3365 non-null   object\n",
      " 23  points_for_away                 3365 non-null   object\n",
      " 24  points_against_away             3365 non-null   object\n",
      " 25  points_differential_away        3365 non-null   object\n",
      " 26  margin_of_victory_away          3365 non-null   object\n",
      " 27  strength_of_schedule_away       3365 non-null   object\n",
      " 28  simple_rating_away              3365 non-null   object\n",
      " 29  offensive_ranking_away          3365 non-null   object\n",
      " 30  defensive_ranking_away          3365 non-null   object\n",
      " 31  playoffs_away                   3365 non-null   object\n",
      " 32  sb_winner_away                  3365 non-null   object\n",
      " 33  points                          3365 non-null   object\n",
      " 34  yards                           3365 non-null   object\n",
      " 35  turnovers                       3365 non-null   object\n",
      " 36  win                             3365 non-null   object\n",
      " 37  loss_home                       3365 non-null   object\n",
      " 38  tie                             3365 non-null   object\n",
      " 39  points_away                     3365 non-null   object\n",
      " 40  yards_away                      3365 non-null   object\n",
      " 41  turnovers_away                  3365 non-null   object\n",
      " 42  win_away                        3365 non-null   object\n",
      " 43  loss_away                       3365 non-null   object\n",
      " 44  tie_away                        3365 non-null   object\n",
      "dtypes: object(45)\n",
      "memory usage: 1.2+ MB\n"
     ]
    }
   ],
   "source": [
    "training.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274a530f",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "We can now begin building the Random Forest model, but first, the categorical features need to be one-hot encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0a983b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "training = pd.get_dummies(training, columns=['team_name_home', 'team_name_away'])\n",
    "validation = pd.get_dummies(validation, columns=['team_name_home', 'team_name_away'])\n",
    "test = pd.get_dummies(test, columns=['team_name_home', 'team_name_away'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff54b2a",
   "metadata": {},
   "source": [
    "Numerical values have to be scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "219bd4ca",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'false'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m X_test, y_test = test.drop(columns=[target]), test[target]\n\u001b[32m      8\u001b[39m scaler = StandardScaler()\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[43mscaler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m### SCALING X \u001b[39;00m\n\u001b[32m     13\u001b[39m X_train_scaled = scaler.transform(X_train)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/nfl-attendance-prediction/venv/lib/python3.13/site-packages/sklearn/preprocessing/_data.py:894\u001b[39m, in \u001b[36mStandardScaler.fit\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m    892\u001b[39m \u001b[38;5;66;03m# Reset internal state before fitting\u001b[39;00m\n\u001b[32m    893\u001b[39m \u001b[38;5;28mself\u001b[39m._reset()\n\u001b[32m--> \u001b[39m\u001b[32m894\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpartial_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/nfl-attendance-prediction/venv/lib/python3.13/site-packages/sklearn/base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/nfl-attendance-prediction/venv/lib/python3.13/site-packages/sklearn/preprocessing/_data.py:930\u001b[39m, in \u001b[36mStandardScaler.partial_fit\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m    898\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Online computation of mean and std on X for later scaling.\u001b[39;00m\n\u001b[32m    899\u001b[39m \n\u001b[32m    900\u001b[39m \u001b[33;03mAll of X is processed as a single batch. This is intended for cases\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    927\u001b[39m \u001b[33;03m    Fitted scaler.\u001b[39;00m\n\u001b[32m    928\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    929\u001b[39m first_call = \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mn_samples_seen_\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m930\u001b[39m X = \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    933\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcsr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcsc\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    934\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    935\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mallow-nan\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    936\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfirst_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    937\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    938\u001b[39m n_features = X.shape[\u001b[32m1\u001b[39m]\n\u001b[32m    940\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/nfl-attendance-prediction/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2944\u001b[39m, in \u001b[36mvalidate_data\u001b[39m\u001b[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[39m\n\u001b[32m   2942\u001b[39m         out = X, y\n\u001b[32m   2943\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[32m-> \u001b[39m\u001b[32m2944\u001b[39m     out = \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mX\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2945\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[32m   2946\u001b[39m     out = _check_y(y, **check_params)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/nfl-attendance-prediction/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:973\u001b[39m, in \u001b[36mcheck_array\u001b[39m\u001b[34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[39m\n\u001b[32m    968\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pandas_requires_conversion:\n\u001b[32m    969\u001b[39m     \u001b[38;5;66;03m# pandas dataframe requires conversion earlier to handle extension dtypes with\u001b[39;00m\n\u001b[32m    970\u001b[39m     \u001b[38;5;66;03m# nans\u001b[39;00m\n\u001b[32m    971\u001b[39m     \u001b[38;5;66;03m# Use the original dtype for conversion if dtype is None\u001b[39;00m\n\u001b[32m    972\u001b[39m     new_dtype = dtype_orig \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m dtype\n\u001b[32m--> \u001b[39m\u001b[32m973\u001b[39m     array = \u001b[43marray\u001b[49m\u001b[43m.\u001b[49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_dtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    974\u001b[39m     \u001b[38;5;66;03m# Since we converted here, we do not need to convert again later\u001b[39;00m\n\u001b[32m    975\u001b[39m     dtype = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/nfl-attendance-prediction/venv/lib/python3.13/site-packages/pandas/core/generic.py:6643\u001b[39m, in \u001b[36mNDFrame.astype\u001b[39m\u001b[34m(self, dtype, copy, errors)\u001b[39m\n\u001b[32m   6637\u001b[39m     results = [\n\u001b[32m   6638\u001b[39m         ser.astype(dtype, copy=copy, errors=errors) \u001b[38;5;28;01mfor\u001b[39;00m _, ser \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.items()\n\u001b[32m   6639\u001b[39m     ]\n\u001b[32m   6641\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   6642\u001b[39m     \u001b[38;5;66;03m# else, only a single dtype is given\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m6643\u001b[39m     new_data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_mgr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6644\u001b[39m     res = \u001b[38;5;28mself\u001b[39m._constructor_from_mgr(new_data, axes=new_data.axes)\n\u001b[32m   6645\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m res.__finalize__(\u001b[38;5;28mself\u001b[39m, method=\u001b[33m\"\u001b[39m\u001b[33mastype\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/nfl-attendance-prediction/venv/lib/python3.13/site-packages/pandas/core/internals/managers.py:430\u001b[39m, in \u001b[36mBaseBlockManager.astype\u001b[39m\u001b[34m(self, dtype, copy, errors)\u001b[39m\n\u001b[32m    427\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m using_copy_on_write():\n\u001b[32m    428\u001b[39m     copy = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m430\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    431\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mastype\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    432\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    433\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    434\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    435\u001b[39m \u001b[43m    \u001b[49m\u001b[43musing_cow\u001b[49m\u001b[43m=\u001b[49m\u001b[43musing_copy_on_write\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    436\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/nfl-attendance-prediction/venv/lib/python3.13/site-packages/pandas/core/internals/managers.py:363\u001b[39m, in \u001b[36mBaseBlockManager.apply\u001b[39m\u001b[34m(self, f, align_keys, **kwargs)\u001b[39m\n\u001b[32m    361\u001b[39m         applied = b.apply(f, **kwargs)\n\u001b[32m    362\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m363\u001b[39m         applied = \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    364\u001b[39m     result_blocks = extend_blocks(applied, result_blocks)\n\u001b[32m    366\u001b[39m out = \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).from_blocks(result_blocks, \u001b[38;5;28mself\u001b[39m.axes)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/nfl-attendance-prediction/venv/lib/python3.13/site-packages/pandas/core/internals/blocks.py:758\u001b[39m, in \u001b[36mBlock.astype\u001b[39m\u001b[34m(self, dtype, copy, errors, using_cow, squeeze)\u001b[39m\n\u001b[32m    755\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCan not squeeze with more than one column.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    756\u001b[39m     values = values[\u001b[32m0\u001b[39m, :]  \u001b[38;5;66;03m# type: ignore[call-overload]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m758\u001b[39m new_values = \u001b[43mastype_array_safe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    760\u001b[39m new_values = maybe_coerce_values(new_values)\n\u001b[32m    762\u001b[39m refs = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/nfl-attendance-prediction/venv/lib/python3.13/site-packages/pandas/core/dtypes/astype.py:237\u001b[39m, in \u001b[36mastype_array_safe\u001b[39m\u001b[34m(values, dtype, copy, errors)\u001b[39m\n\u001b[32m    234\u001b[39m     dtype = dtype.numpy_dtype\n\u001b[32m    236\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m237\u001b[39m     new_values = \u001b[43mastype_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m):\n\u001b[32m    239\u001b[39m     \u001b[38;5;66;03m# e.g. _astype_nansafe can fail on object-dtype of strings\u001b[39;00m\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m#  trying to convert to float\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m errors == \u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/nfl-attendance-prediction/venv/lib/python3.13/site-packages/pandas/core/dtypes/astype.py:182\u001b[39m, in \u001b[36mastype_array\u001b[39m\u001b[34m(values, dtype, copy)\u001b[39m\n\u001b[32m    179\u001b[39m     values = values.astype(dtype, copy=copy)\n\u001b[32m    181\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m182\u001b[39m     values = \u001b[43m_astype_nansafe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    184\u001b[39m \u001b[38;5;66;03m# in pandas we don't store numpy str dtypes, so convert to object\u001b[39;00m\n\u001b[32m    185\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, np.dtype) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(values.dtype.type, \u001b[38;5;28mstr\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/nfl-attendance-prediction/venv/lib/python3.13/site-packages/pandas/core/dtypes/astype.py:133\u001b[39m, in \u001b[36m_astype_nansafe\u001b[39m\u001b[34m(arr, dtype, copy, skipna)\u001b[39m\n\u001b[32m    129\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[32m    131\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mor\u001b[39;00m arr.dtype == \u001b[38;5;28mobject\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m dtype == \u001b[38;5;28mobject\u001b[39m:\n\u001b[32m    132\u001b[39m     \u001b[38;5;66;03m# Explicit copy, or required since NumPy can't view from / to object.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m133\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m arr.astype(dtype, copy=copy)\n",
      "\u001b[31mValueError\u001b[39m: could not convert string to float: 'false'"
     ]
    }
   ],
   "source": [
    "target = 'weekly_attendance_home'\n",
    "\n",
    "# Separate features and labels\n",
    "X_train, y_train = training.drop(columns=[target]), training[target]\n",
    "X_val, y_val = validation.drop(columns=[target]), validation[target]\n",
    "X_test, y_test = test.drop(columns=[target]), test[target]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "### SCALING X \n",
    "\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "### SCALING Y \n",
    "target_scaler = StandardScaler()\n",
    "target_scaler.fit(y_train.values.reshape(-1, 1))\n",
    "\n",
    "y_train_scaled = target_scaler.transform(y_train.values.reshape(-1, 1)).flatten()\n",
    "y_val_scaled = target_scaler.transform(y_val.values.reshape(-1, 1)).flatten()\n",
    "y_test_scaled = target_scaler.transform(y_test.values.reshape(-1, 1)).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca53572",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
